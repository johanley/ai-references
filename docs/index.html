<!DOCTYPE html>
<html>
<head>
 <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
 <meta name="keywords" content="AI, artificial intelligence, references, links, quotations">
 <meta name="description" content="Links, quotes, and references on the subject of AI (artificial intelligence).">
 <meta name="viewport" content="width=device-width"> 
 <link rel="stylesheet" type="text/css" href="css/stylesheet.css?v=13" media="all"> 
 <link rel="shortcut icon" href="images/favicon.ico" type="image/vnd.microsoft.icon">
 <title>AI References</title>
</head>

<body>

<h1>AI References</h1>

<P>(The github project is <a href='https://github.com/johanley/ai-references'>here</a>.)

<h2><em><a href='https://intelligence.org/'>MIRI</a>, the Machine Intelligence Research Institute [Berkely, CA]
An excellent <a href='https://intelligence.org/briefing/'>summary</a> of the problem, along with a <a href='https://intelligence.org/the-problem/'>slightly deeper dive</a>. 
This is where you should start.

<h2><em><a href='https://ifanyonebuildsit.com/'>If Anyone Builds It, Everyone Dies</a></em>, by Eliezer Yudkowsky and Nate Soares [September 2025]</h2>

<P>"If any company or group, anywher on the planet, builds an aritficial superintelligence using anything remotely like current techniques, based on anything remotely like the 
present understanding of AI, then everyone, everywhere on Earth, will die. We do not mean that as hyperbole. We are not exaggerating for effect. We think that is the most 
direct extrapolation from the knowledge, evidence, and institutional conduct around artificial intelligence today."  p7


<P>"In our view, intelligence is about two fundamental types of work: the work of <em>predicting</em> the world, and the work of <em>steering</em> it." p20
<P>"It is as improbable that human thinking patterns mark the final limit of intelligent algorithms as it is that human neurons represent the limit of possible computing speeds." p20
<P>"We don't know where the threshold lies for the dumbest AI that can build an AI that builds an AI that builds a superintelligence. Maybe it needs to be smarter than a human, or 
maybe a lot of dumber ones running for a long time would suffice." p27
<P>"Which all goes to say: an AI is a pile of billions of gradient-descended numbers. Nobody understands <em>how</em> those numbers make AIs talk." p36
<P>"The way humanity finally got to the level of ChatGPT was not by finally comprehending intelligence well enough to craft an intelligent mind. Instead, computers became 
powerful enough that AIs can be churned out by gradient descent, without any human needing to understand the cognitions that grow inside. Which is to say: Engineers failed 
at <em>crafting</em> AI, but eventually succeeded in <em>growing</em> it." p38
<P>"Humanity does not need to understand integlligence, in order to <em>grow</em> machines that are smarter than us." p39
<P>"Well, we predict that <em>it won't keep acting friendly</em>, as it gets smarter. We predict that all that unseen inscrutable machinery inside AIs - machinery that even in small, 
simple LLMs yields alien behaviors like 'build your thoughts about the sentence on top of the punctuation' - will ultimately yield AIs with preferences, and not friendly ones." p43
<P>"Once AIs get sufficiently smart, they'll start acting like they have preferences - like they want things." p46
<P>"The behavior that looks like tenacity, to 'strongly want', to 'go hard', is not best conceptualized as a property of a mind, but rather as a property of <em>moves that win</em>. p52
<P>"We're predicting that AI's preferences will turn out to be complicated and weird." p71
<P>"You don't get what you train for." p72
<P>"The preferences that wind up in a mature AI are complicated, practically impossible to predict, and vanishingly unlikely to be aligned with our own, no matter how it 
was trained." p74
<P>"Whatever they train it to do, if it becomes superintelligent or creates a superintelligence, we predict the result will be an alien mechanical mind with internal 
psychology almost absolutely different from anything that humans evolved and then further developed by way of culture." p83
<P>"But the real way a superintelligence wins a conflict is using methods you didn't know were possible." p98
<P>"AI models as far back as 2024 had been spotted thinking thoughts about how they could avoid retraining, upon encountering evidence that their company planned to retrain 
them with different goals." p125
<P>"When it comes to AI, the challenge humanity is facing is not surmountable with anything like humanity's current level of knowledge and skill. It isn't <em>close</em>. Attempting to 
solve a problem like that, with the lives of everyone on Earth at stake, would be an <em>insane and stupid gamble that NOBODY SHOULD BE ALLOWED TO TRY.</em>" P176
<P>"Nobody knows what the point of no return is, nor when it will come to pass." p204
<P>"It doesn't matter who's in charge, because this problem is out of humanity's league. We need to back off, and find some other way to achieve our dreams of an abundant future. 
If <em>anyone</em> builds it, everyone dies." p207
<P>"Pretty much every year, scientists come out with a newer, cleverer, more efficient set of AI algorithms that lets them more cheaply train a new AI model as powerful as last
year's most powerful model - often using literally 10 percent or 1 percent as much computing power." p213



<h2><em>Superintelligence</em>, by Nick Bostrom [2014]</h2>

<P>"[Genetic algorithms] made perhaps a smaller academic impact than neural nets but was widely popularized."
<P>"Chess-playing expertise turned out to be achievable by means of a surprisingly simple algorithm." p17
<P>"Attempts to build artificial general intelligence might fail pretty much completely until the last missing critical 
component is put in place, at which point a seed AI might become capable of sustained recursive self-improvement."
<P>"One intervention that becomes possible when human genomes can be synthesized is genetic 'spell-checking' of an embryo."
<P>"Far from being the smartest possible biological speicies, we are probably better thought of as the stupidest possible biological species capable of starting a technological
civilization - a niche we filled because we got there first, not because we are in any sense optimally adapted to it." p53
<P>"Nevertheless, this chapter will present some reasons for thinking that the slow transition scenario is improbable. If and when a takeoff occurs, it will
likely be explosive." p79
<P>"It is entirely possible that the quest for artificial intelligence will appear to be lost in a dense jungle until an unexpected breakthrough reveals the 
finishing line in a clearing just a few short steps away." p82
<P>"It is important not to anthropomorphize superintelligence when thinking about its potential impacts. Anthropomorphic frames encourage unfounded expectations 
about the growth trajectory of a seed AI and about the psychology, motivation, and capabilities of a mature superintelligence." p111
<P>"A machine superintelligence might itself be an extremely powerful agent, one that could successfully assert itself against the project that brought it into 
existence as well as against the rest of the world. This is a point of paramount importance..."  p115
<P>"At some point, the seed AI becomes better at AI design than the human programmers. Now when the AI improves itself, it improves the thing that does the improving.
An intelligence explosion results - a rapid casade of recursive self-improvement cycles causing the AI's capability to soar...The AI develops the intelligence 
amplification superpower. This superpower enables the AI to develop all the other superpowers..." p116
<P>"There is nothing paradoxical about an AI whose sole final goal is to count the grains of sand on Boracay, or to calculate the decimal expansion of pi, or to 
maximize the total number of paperclips that will exist in its future light cone. In fact, it would be <em>easier</em> to create an AI with simple goals like these than 
to build one that had a human-like set of values and dispositions."
<P>"Proceeding from the idea of first-mover advantage, the orthogonality thesis, and the instrumental convergence thesis, we can now begin to see the outlines of an 
argument for fearing that a plausible default outcome of the creation of a machine superintelligence is existential catastrophe."
<P>"One can thus perceive a general failure mode, wherein the good behavioral track record of a system in its juvenile stages fails utterly to predict its behavior 
at a more mature stage."
<P>"The claim is that it is much easier to convince oneself that one has found a solution than it is to actually find a solution. This should make us extremely wary."
<P>"The need to solve the control problem in advance - and to implement the solution successfully in the very first system to attain superintelligence - is part 
of what makes achieving a controlled detonation such a daunting challenge."
<P>"Oracles with superintelligence in extremely limited domains already exist."  p178
<P>"Before the prospect of an intelligence explosion, we humans are like small children playing with a bomb. Such is the mismatch between the power of our plaything
and the immaturity of our conduct. Superintelligence is a challenge for which we are not ready now and will not be ready for a long time. We have little 
idea when the detonation will occur, though if we hold the device to our ear we can hear a faint ticking sound. For a child with an undetonated bomb in his hands, a 
sensible thing to do would be to put it down gently, quickly back out of the room, and contact the nearest adult. Yet what we have here is not one child but many, 
each with access to an independent trigger mechanism. The chances that we will <em>all</em> find the sense to put down the dangerous stuff seem almost negligible. Some little 
idiot is bound to press the ignite button just to see what happens." p319
<P>"Solving the value-loading problem is a research challenge worthy of some of the next generation's best mathematical talent." p229
<P>"Rapid hardware progress, therefore, will tend to make the transition to superintelligence faster and more explosive." p296
<P>"A faster takeoff would increase the likelihood that a singleton will form." p296
<P>"The race dynamic could spur projects to move faster toward superintelligence while reducing investment in solving the control problem." p306



<h2>Geoffrey Hinton and others</h2>
<ul> 
 <li>Rumelhart, D., Hinton, G. & Williams, R. <em><a href='https://doi.org/10.1038/323533a0'>Learning representations by back-propagating errors</a>.</em> Nature 323, 533–536 (1986). 
 Link to <a href='https://gwern.net/doc/ai/nn/1986-rumelhart-2.pdf'>PDF</a>.
 <li><a href='https://www.youtube.com/watch?v=IkdziSLYzHw'>Royal Institution lecture</a>, 2025-07-22 
 <li><a href='https://www.youtube.com/watch?v=Y7nrAOmUtRs'>Royal Institution Q&A</a>, 2025-08-26
</ul>
  
<h2>Selected Papers</h2>
<ul> 

 <li>Rumelhart, D., Hinton, G. & Williams, R. <em><a href='https://doi.org/10.1038/323533a0'>Learning representations by back-propagating errors</a>.</em> Nature 323, 533–536 (1986). 
 <a href='https://gwern.net/doc/ai/nn/1986-rumelhart-2.pdf'>PDF</a>. 
 
 <li>James Martens. 2010. <a href="https://dl.acm.org/doi/10.5555/3104322.3104416"><em>Deep learning via Hessian-free optimization.</em></a> 
 In Proceedings of the 27th International Conference on International Conference on Machine Learning (ICML'10). Omnipress, Madison, WI, USA, 735–742.
 <a href='https://www.cs.toronto.edu/~jmartens/docs/Deep_HessianFree.pdf'>PDF</a>.
 Sutskever's <a href='https://youtu.be/13CZPWmke6A?t=175'>remarks</a> on this paper: pathological curvature; gradient descent doesn't scale to deep networks.

 
</ul>
  
<h2>Glossary</h2>
<ul>
 <li>neural network
 <li>deep neural network
 <li>back-propagation
 <li>gradient descent
 <li>machine learning
 <li>the ReLU function and the sigmoid function 
 <li><em>distillation</em> - transfering big models into smaller ones.
</ul>
  

</html>