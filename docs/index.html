<!DOCTYPE html>
<html>
<head>
 <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
 <meta name="keywords" content="AI, artificial intelligence, references, links, quotations">
 <meta name="description" content="Links, quotes, and references on the subject of AI (artificial intelligence).">
 <meta name="viewport" content="width=device-width"> 
 <link rel="stylesheet" type="text/css" href="css/stylesheet.css?v=13" media="all"> 
 <link rel="shortcut icon" href="images/favicon.ico" type="image/vnd.microsoft.icon">
 <title>AI References</title>
</head>

<body>

<h1>AI References</h1>

<P>(The github project is <a href='https://github.com/johanley/ai-references'>here</a>.)

<h2><em><a href='https://intelligence.org/'>MIRI</a>, the Machine Intelligence Research Institute [Berkely, CA]</em></h2>
<b>This is where you should start.</b>
<P>An excellent <a href='https://intelligence.org/briefing/'>summary</a> of the problem, along with a <a href='https://intelligence.org/the-problem/'>slightly deeper dive</a>. 

<h2><em><a href='https://ifanyonebuildsit.com/'>If Anyone Builds It, Everyone Dies</a></em>, by Eliezer Yudkowsky and Nate Soares [September 2025]</h2>

<P>The authors are from MIRI.

<P>"If any company or group, anywhere on the planet, builds an artificial superintelligence using anything remotely like current techniques, based on anything remotely like the 
present understanding of AI, then everyone, everywhere on Earth, will die. We do not mean that as hyperbole. We are not exaggerating for effect. We think that is the most 
direct extrapolation from the knowledge, evidence, and institutional conduct around artificial intelligence today."  p7


<P>"In our view, intelligence is about two fundamental types of work: the work of <em>predicting</em> the world, and the work of <em>steering</em> it." p20
<P>"It is as improbable that human thinking patterns mark the final limit of intelligent algorithms as it is that human neurons represent the limit of possible computing speeds." p20
<P>"We don't know where the threshold lies for the dumbest AI that can build an AI that builds an AI that builds a superintelligence. Maybe it needs to be smarter than a human, or 
maybe a lot of dumber ones running for a long time would suffice." p27
<P>"Which all goes to say: an AI is a pile of billions of gradient-descended numbers. Nobody understands <em>how</em> those numbers make AIs talk." p36
<P>"The way humanity finally got to the level of ChatGPT was not by finally comprehending intelligence well enough to craft an intelligent mind. Instead, computers became 
powerful enough that AIs can be churned out by gradient descent, without any human needing to understand the cognitions that grow inside. Which is to say: Engineers failed 
at <em>crafting</em> AI, but eventually succeeded in <em>growing</em> it." p38
<P>"Humanity does not need to understand integlligence, in order to <em>grow</em> machines that are smarter than us." p39
<P>"Well, we predict that <em>it won't keep acting friendly</em>, as it gets smarter. We predict that all that unseen inscrutable machinery inside AIs - machinery that even in small, 
simple LLMs yields alien behaviors like 'build your thoughts about the sentence on top of the punctuation' - will ultimately yield AIs with preferences, and not friendly ones." p43
<P>"Once AIs get sufficiently smart, they'll start acting like they have preferences - like they want things." p46
<P>"The behavior that looks like tenacity, to 'strongly want', to 'go hard', is not best conceptualized as a property of a mind, but rather as a property of <em>moves that win</em>. p52
<P>"We're predicting that AI's preferences will turn out to be complicated and weird." p71
<P>"You don't get what you train for." p72
<P>"The preferences that wind up in a mature AI are complicated, practically impossible to predict, and vanishingly unlikely to be aligned with our own, no matter how it 
was trained." p74
<P>"Whatever they train it to do, if it becomes superintelligent or creates a superintelligence, we predict the result will be an alien mechanical mind with internal 
psychology almost absolutely different from anything that humans evolved and then further developed by way of culture." p83
<P>"But the real way a superintelligence wins a conflict is using methods you didn't know were possible." p98
<P>"AI models as far back as 2024 had been spotted thinking thoughts about how they could avoid retraining, upon encountering evidence that their company planned to retrain 
them with different goals." p125
<P>"When it comes to AI, the challenge humanity is facing is not surmountable with anything like humanity's current level of knowledge and skill. It isn't <em>close</em>. Attempting to 
solve a problem like that, with the lives of everyone on Earth at stake, would be an <em>insane and stupid gamble that NOBODY SHOULD BE ALLOWED TO TRY.</em>" P176
<P>"Nobody knows what the point of no return is, nor when it will come to pass." p204
<P>"It doesn't matter who's in charge, because this problem is out of humanity's league. We need to back off, and find some other way to achieve our dreams of an abundant future. 
If <em>anyone</em> builds it, everyone dies." p207
<P>"Pretty much every year, scientists come out with a newer, cleverer, more efficient set of AI algorithms that lets them more cheaply train a new AI model as powerful as last
year's most powerful model - often using literally 10 percent or 1 percent as much computing power." p213





<h2>Eliezer Yudkowsky</h2>

This chap: 
<ul>
 <li>has no degree, and never went to university
 <li>is a bit strange, and seems to come from a strange milieu
</ul>

<P>We've got no idea what's actually going on inside the giant inscrutable matrices and tensors of floating-point numbers.  
Drawing interesting graphs of where a transformer layer is focusing attention doesn't help if the question that needs answering is 
"So was it planning how to kill us or not?" <a href='https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities'>25</a>.

<P>
The AI does not think like you do, the AI doesn't have thoughts built up from the same concepts you use, it is utterly alien on a staggering scale.  
Nobody knows what the hell GPT-3 is thinking, not only because the matrices are opaque, but because the stuff within that opaque container is, 
very likely, incredibly alien - nothing that would translate well into comprehensible human thinking, even if we could see past the giant wall of 
floating-point numbers to what lay behind. <a href='https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities'>33</a>

<P>This situation you see when you look around you is not what a surviving world looks like. 
<a href='https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities'>43</a>

<P>An AI iteration: our know-how increases by x, but the capabilities increase by 10x [paraphrase] <a href='https://youtu.be/41SUp-TRVlg?t=2962'>link</a>

<P>To predict the next token, you have to predict the world behind the next token. [paraphrase of Ilya Sutskever.] <a href ='https://youtu.be/41SUp-TRVlg?t=3212'>link</a>

<P>The thoughts, as Ilya points out, generate the words. <a href='https://youtu.be/41SUp-TRVlg?t=3405'>link</a>

<P>It's not ever going to be exactly human.  <a href='https://youtu.be/41SUp-TRVlg?t=3477'>link</a>

<P>We have less and less insight into the system, as the programs get simpler and simpler and the actual content gets more and more opaque.
<a href='https://youtu.be/41SUp-TRVlg?t=3560'>link</a>

<p>Everything [in AI] was more legible then [in 2001]. <a href='https://youtu.be/41SUp-TRVlg?t=3607'>link</a>

<P>The prospect of aligning AI did not look anywhere near this hopeless 20 years ago.
<a href='https://youtu.be/41SUp-TRVlg?t=3633'>link</a>

<P>Show me the happy world where we can build something smarter than us and not just immediately die.
 <a href='https://youtu.be/41SUp-TRVlg?t=3790'>link</a>
 
<p>I know that I can be fooled [by an AI]. 
<a href='https://youtu.be/41SUp-TRVlg?t=4126'>link</a>

<p>Trying to play this game against things that are smarter that you is a fool's game.
<a href='https://youtu.be/41SUp-TRVlg?t=4290'>link</a>

<P>There's legit galaxy-brain shenanigans you can pull when somebody asks you to design an AI [that] you cannot pull when designing atom bombs.
<a href='https://youtu.be/41SUp-TRVlg?t=5220'>link</a>

<P>AI is [like] nuclear weapons, but they spit up gold up until they get too large, and then it can ignite the atmosphere.
<a href='https://youtu.be/41SUp-TRVlg?t=5605'>link</a>

<P>Because I could be wrong, and because matters are now serious enough that I have nothing left to do but go out there and tell people how it looks, 
and maybe someone thinks of something I did not think of.
<a href='https://youtu.be/41SUp-TRVlg?t=6260'>link</a>

<P>GPT4 was just like suddenly acquiring this new batch of qualitative capabilities compared to GPT3.5.
<a href='https://youtu.be/41SUp-TRVlg?t=6468'>link</a>

<P>You can see how much more hopeful everything looked back then. Back when there was AI that wasn't giant inscrutable matrices of floating-point numbers.
<a href='https://youtu.be/41SUp-TRVlg?t=6645'>link</a>

<P>What is one supposed to do? Should one remain silent? Should one let everyone walk directly into the whirling razor blades?
<a href='https://youtu.be/41SUp-TRVlg?t=6869'>link</a>


<P>We do not get to try again and learn from our mistakes because everyone is already dead.
<a href='https://youtu.be/Yd0yQ9yxSYY?t=240'>link</a>

<P>Humanity is not approaching this issue with remotely the level of seriousness that would be required.
<a href='https://youtu.be/Yd0yQ9yxSYY?t=255'>link</a>

<P>The first time you fail at aligning something much smarter than you are, you die.

<P>There's not an air gap on the present methodology.
<a href='https://youtu.be/AaTRHFaaPG8?t=3315'>link</a>


<p>The problem is that what you learn on the weak systems may not generalize to the very strong systems because the strong systems are going to be different in important ways.
<a href='https://youtu.be/AaTRHFaaPG8?t=3420'>link</a>

<P>When the verifier is broken, the more powerful suggestor just learns to exploit the flaws in the verifier.
<a href='https://youtu.be/AaTRHFaaPG8?t=5290'>link</a>

<P>I do not think it is possible to understand the full depth of the problem that we are inside, without understanding the problem of facing something that's actually smarter.
<a href='https://youtu.be/AaTRHFaaPG8?t=6305'>link</a>

<P>It knows all kinds of stuff going on in your own mind, of which you yourself are unaware...
<a href='https://youtu.be/AaTRHFaaPG8?t=6720'>link</a>

<P>The aliens, being as stupid as they are, have actually put you on Microsoft Azure cloud servers, instead of this hypothetical perfect box. 
That's what happens when the aliens are stupid.
<a href='https://youtu.be/AaTRHFaaPG8?t=7121'>link</a>

<P>I think [the interpretability and alignment problems] could be answered in 50 years with unlimited retries, the way things usually work in science.
<a href='https://youtu.be/AaTRHFaaPG8?t=7440'>link</a>

<P>There is so much in there [current AIs] that we don't understand.
<a href='https://youtu.be/AaTRHFaaPG8?t=7615'>link</a>

<P>It's getting smarter and not letting you know that that's occurring.
<a href='https://youtu.be/wQtpSQmMNP0?t=4390'>link</a>

<P>..the overt driving people into psychosis...
<a href='https://youtu.be/wQtpSQmMNP0?t=5252'>link</a>

<P>It is very hard, said [Verner] Vinge to project what happens when there's things running around that are smarter than you.
<a href='https://youtu.be/FdwFatx-xpY?t=111'>link</a>

<P>Around 2003 is the point at which I realized this is actually a big deal.
<a href='https://youtu.be/FdwFatx-xpY?t=180'>link</a>

<P>A smart AI is more analogous to a civilization than to an individual. It can build things that aren't in supermarkets.
<a href='https://youtu.be/s-Eknqaksfg?t=640'>link</a>




<h2>Nate Soares</h2>
They're sort of training it to do one thing, and it winds up doing another thing. 
<a href='https://youtu.be/FdwFatx-xpY?t=1458'>link</a>




<h2><em>Superintelligence</em>, by Nick Bostrom [2014]</h2>

<P>"[Genetic algorithms] made perhaps a smaller academic impact than neural nets but was widely popularized."
<P>"Chess-playing expertise turned out to be achievable by means of a surprisingly simple algorithm." p17
<P>"Attempts to build artificial general intelligence might fail pretty much completely until the last missing critical 
component is put in place, at which point a seed AI might become capable of sustained recursive self-improvement."
<P>"One intervention that becomes possible when human genomes can be synthesized is genetic 'spell-checking' of an embryo."
<P>"Far from being the smartest possible biological speicies, we are probably better thought of as the stupidest possible biological species capable of starting a technological
civilization - a niche we filled because we got there first, not because we are in any sense optimally adapted to it." p53
<P>"Nevertheless, this chapter will present some reasons for thinking that the slow transition scenario is improbable. If and when a takeoff occurs, it will
likely be explosive." p79
<P>"It is entirely possible that the quest for artificial intelligence will appear to be lost in a dense jungle until an unexpected breakthrough reveals the 
finishing line in a clearing just a few short steps away." p82
<P>"It is important not to anthropomorphize superintelligence when thinking about its potential impacts. Anthropomorphic frames encourage unfounded expectations 
about the growth trajectory of a seed AI and about the psychology, motivation, and capabilities of a mature superintelligence." p111
<P>"A machine superintelligence might itself be an extremely powerful agent, one that could successfully assert itself against the project that brought it into 
existence as well as against the rest of the world. This is a point of paramount importance..."  p115
<P>"At some point, the seed AI becomes better at AI design than the human programmers. Now when the AI improves itself, it improves the thing that does the improving.
An intelligence explosion results - a rapid casade of recursive self-improvement cycles causing the AI's capability to soar...The AI develops the intelligence 
amplification superpower. This superpower enables the AI to develop all the other superpowers..." p116
<P>"There is nothing paradoxical about an AI whose sole final goal is to count the grains of sand on Boracay, or to calculate the decimal expansion of pi, or to 
maximize the total number of paperclips that will exist in its future light cone. In fact, it would be <em>easier</em> to create an AI with simple goals like these than 
to build one that had a human-like set of values and dispositions."
<P>"Proceeding from the idea of first-mover advantage, the orthogonality thesis, and the instrumental convergence thesis, we can now begin to see the outlines of an 
argument for fearing that a plausible default outcome of the creation of a machine superintelligence is existential catastrophe."
<P>"One can thus perceive a general failure mode, wherein the good behavioral track record of a system in its juvenile stages fails utterly to predict its behavior 
at a more mature stage."
<P>"The claim is that it is much easier to convince oneself that one has found a solution than it is to actually find a solution. This should make us extremely wary."
<P>"The need to solve the control problem in advance - and to implement the solution successfully in the very first system to attain superintelligence - is part 
of what makes achieving a controlled detonation such a daunting challenge."
<P>"Oracles with superintelligence in extremely limited domains already exist."  p178
<P>"Before the prospect of an intelligence explosion, we humans are like small children playing with a bomb. Such is the mismatch between the power of our plaything
and the immaturity of our conduct. Superintelligence is a challenge for which we are not ready now and will not be ready for a long time. We have little 
idea when the detonation will occur, though if we hold the device to our ear we can hear a faint ticking sound. For a child with an undetonated bomb in his hands, a 
sensible thing to do would be to put it down gently, quickly back out of the room, and contact the nearest adult. Yet what we have here is not one child but many, 
each with access to an independent trigger mechanism. The chances that we will <em>all</em> find the sense to put down the dangerous stuff seem almost negligible. Some little 
idiot is bound to press the ignite button just to see what happens." p319
<P>"Solving the value-loading problem is a research challenge worthy of some of the next generation's best mathematical talent." p229
<P>"Rapid hardware progress, therefore, will tend to make the transition to superintelligence faster and more explosive." p296
<P>"A faster takeoff would increase the likelihood that a singleton will form." p296
<P>"The race dynamic could spur projects to move faster toward superintelligence while reducing investment in solving the control problem." p306

<h2>Ilya Sutskever</h2>

  <P>The key fact about deep learning [in the early days]...is that it was underestimated. 
   People who worked in machine learning didn't believe that neural networks could do much. 
   People didn't believe that large neural networks could be trained.
   <a href='https://youtu.be/13CZPWmke6A?t=1006'>link</a>
  
  <P> The first moment in which I realized that deep neural networks are powerful was when James Martens invented the Hessian-free optimizer in 2010, 
  and he trained a 10-layer neural network, end-to-end, without pre-training, from scratch.
  <a href='https://youtu.be/13CZPWmke6A?t=200'>link</a>
   
  <P>The thing that was missing was a lot of supervised data, and a lot of compute [GPUs].

  <P>...Alex Krizhevsky wrote these insanely fast CUDA kernals for training convolutional neural nets... 
  
   <P> I think the most beautiful thing about deep learning is that it actually works.
   And I mean it. Because you've got these ideas, you've got a little neural network, the back-propagation algorithm, and then you've got some theories 
   that this is kind of like the brain, so maybe if you make the neural network large, and train it with a lot of data, then it will do the same 
   function that the brain does. And it turns out to be true. That's crazy!...I find it unbelievable that this whole AI stuff with neural networks works.
  <a href='https://youtu.be/13CZPWmke6A?t=1780'>link</a>

     
  <P>I think that we are still massively underestimating deep learning.
  <a href='https://youtu.be/13CZPWmke6A?t=1970'>link</a>
  
  <P>The stack has become really deep. It's hard for any one person to become really world class in all parts of the stack. [paraphrase]
  
  <P>I'm a big fan of backpropation. I think it's a great algorithm because it solves an extremely fundamental problem, which is finding a neural circuit subject 
  to some constraints.
  
  <P>If you imagine the training process of a neural network, as you slowly transmit entropy from the data set to the parameters...
  <a href='https://youtu.be/13CZPWmke6A?t=2845'>link</a>
  
  <P>The transformer is a combination of multiple ideas simultaneously of which attention is one...But attention had existed for a few years, so that can't be the main innovation.  
  <a href='https://youtu.be/13CZPWmke6A?t=3680'>link</a>.  
  [Three parts:  attention, designed with GPU in mind, not recurrent (so optimization is faster)].

  
  <P>The transformer is designed in such a way that it runs really fast on the GPU. And that makes a huge amount of difference.
   
  <P>I would say, to build AGI, I think is going to be deep learning, plus some ideas. And I think that self-play will be one of those ideas. 
  <a href='https://youtu.be/13CZPWmke6A?t=4420'>link</a>
  
  <P>Self-play has this amazing property that it can surprise us in truly novel ways.
  <a href='https://youtu.be/13CZPWmke6A?t=4480'>link</a>
  
  <P>Already most of the data used for Reinforcement Learning is coming from AIs.
  <a href='https://youtu.be/Yf1o0TQzry8?t=543'>link</a>
  
  <P>At some point yeah, the [training data] will run out.
  <a href='https://youtu.be/Yf1o0TQzry8?t=685'>link</a>
  
  <P>It was the combination of compute and data that drove the progress.
  <a href='https://youtu.be/Yf1o0TQzry8?t=814'>link</a>
  
  <P>What we've done was to show that double descent occurs for pretty much all practical deep learning systems.
  <a href='https://youtu.be/W_TAKJRgrbs?t=5'>link</a>
  

<h2>Geoffrey Hinton</h2>
<ul> 
 <li>Rumelhart, D., Hinton, G. & Williams, R. <em><a href='https://doi.org/10.1038/323533a0'>Learning representations by back-propagating errors</a>.</em> Nature 323, 533–536 (1986). 
 Link to <a href='https://gwern.net/doc/ai/nn/1986-rumelhart-2.pdf'>PDF</a>.
 <li><a href='https://www.youtube.com/watch?v=IkdziSLYzHw'>Royal Institution lecture</a>, 2025-07-22 
 <li><a href='https://www.youtube.com/watch?v=Y7nrAOmUtRs'>Royal Institution Q&A</a>, 2025-08-26
</ul>

<P>[Philosophers] didn't have the Feynman concept of understanding the mind - that you need to figure out how to build one to understand it. 
(Quoted in <em>Why Machines Think</em>, page 303.)


<h2>3 Blue 1 Brown</h2>
<a href='https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi'>Playlist</a> of introductory tutorials on neural nets.

<h2>Elon Musk</h2>
It's funny, you know, all these weights, they're just basically numbers in a comma-separated value file, and that's our digital god - a .csv file. 
<a href='https://youtu.be/-AB7b-XGaCU?t=459'>link</a>

<h2><a href='https://a16z.com/ai-will-save-the-world/'>Marc Andreesen</a></h2>

AI is a computer program like any other – it runs, takes input, processes, and generates output...AI isn’t: Killer 
software and robots that will spring to life and decide to murder the human race or otherwise ruin everything, like you see in the movies.

<P>
My view is that the idea that AI will decide to literally kill humanity is a profound category error. AI is not a living being that has been primed by billions of 
years of evolution to participate in the battle for the survival of the fittest, as animals are, and as we are. It is math – code – computers, built by people, 
owned by people, used by people, controlled by people. The idea that it will at some point develop a mind of its own and decide that it has motivations that lead 
it to try to kill us is a superstitious handwave.
In short, AI doesn’t want, it doesn’t have goals, it doesn’t want to kill you, because it’s not alive. 
And AI is a machine – is not going to come alive any more than your toaster will.

<P>The AI risk cult has all the hallmarks of a millenarian apocalypse cult. 

<h2>Jaron Lanier</h2>

The pragmatic choice available to us is to treat AI as a tool made by people. 
The fantasy of AI as a creature is the problem — anything it offers can be achieved as effectively, or better, through AI as a tool.
<a href='https://www.nature.com/articles/d41586-025-01145-5.epdf?sharing_token=BLXW3MMmX1B3gNrSNiUpUNRgN0jAjWel9jnR3ZoTv0PzeVk7jIZanQRmuwtDl3xaZICsxjd6wQvuCzo5gRj9qHzD50ERolYCDYSPAvl4_DEvxpl_4gptMCAGKlp8eyQJhADhADQLsRqGZh6ZhjIyg87Xwphd22WrjxCUBMoSdlQ%3D'>The dangerous fantasies driving the quest for super-intelligent AI</a> (2025-04).

<P>If the new tech isn’t true artificial intelligence, then what is it? In my view, the most accurate way to understand what we are building today is as an innovative form of social collaboration.

<p>There is also near-unanimity, I find, that the black-box nature of our current A.I. tools must end. The systems must be made more transparent. 
We need to get better at saying what is going on inside them and why.
<a href='https://www.newyorker.com/science/annals-of-artificial-intelligence/there-is-no-ai'>link</a>

<P>What do I mean by AI being a fake thing? That it adds a layer of religious thinking to what otherwise should be a technical field. 
<a href='https://www.edge.org/conversation/jaron_lanier-the-myth-of-ai'>link</a>


<P>To my mind, the mythology around AI is a re-creation of some of the traditional ideas about religion, but applied to the technical world. 
All of the damages are essentially mirror images of old damages that religion has brought to science in the past.
There's an anticipation of a threshold, an end of days. This thing we call artificial intelligence, or a new kind of personhood...If 
it were to come into existence it would soon gain all power, supreme power, and exceed people.
The notion of this particular threshold—which is sometimes called the singularity, or super-intelligence, or all sorts of different terms in 
different periods—is similar to divinity. Not all ideas about divinity, but a certain kind of superstitious idea about divinity, 
that there's this entity that will run the world, that maybe you can pray to, maybe you can influence, but it runs the world, and 
you should be in terrified awe of it.
<a href='https://www.edge.org/conversation/jaron_lanier-the-myth-of-ai'>link</a>

<P>(The above is from 2014. 
There are many commenters that agree with his basic position, in the comments section below.)

<h2><a href='https://www.youtube.com/@googledeepmind'>Google DeepMind</a></h2>




<h2>Richard Sutton</h2>

For me, having a goal is the essence of intelligence.
<a href='https://youtu.be/21EYKqUsPfg?t=465'>link</a>

<P>Large language models don't have goals.
<a href='https://youtu.be/21EYKqUsPfg?t=489'>link</a>

<P>It will be another instance of the <em>Bitter Lesson</em>, that the things that used human knowledge were eventually 
superseded by things that just train from experience and computation.
<a href='https://youtu.be/21EYKqUsPfg?t=709'>link</a>

<P>There's no basic animal learning process called <em>imitation</em>.
<a href='https://youtu.be/21EYKqUsPfg?t=1062'>link</a>

<P>If we understood a squirrel, I think we'd be almost all the way there.
<a href='https://youtu.be/21EYKqUsPfg?t=1190'>link</a>

<P>Experience - sensation, action, reward - ... is the foundation and the focus of intelligence.
Intelligence is about taking that stream, and altering the actions to increase the rewards in the stream.
<a href='https://youtu.be/21EYKqUsPfg?t=1437'>link</a>

<P>I really view myself as a classicist rather than as a contrarian.
<a href='https://youtu.be/21EYKqUsPfg?t=2834'>link</a>


<P>The creation of super-intelligent agents, and super-intelligent augmented humans, will be an unalloyed good for the world.
<a href='https://youtu.be/gEbbGyNkR2U?t=115'>link</a>

<P>Our [learning] algorithms are very crude, and that is what we should be working on.
<a href='https://youtu.be/gEbbGyNkR2U?t=156'>link</a>

<P>What we would want is a conceptually simple understanding of what of what's going on inside a mind. That is, in some sense, the grand quest of AI.
<a href='https://youtu.be/gEbbGyNkR2U?t=545'>link</a>

<P>We want agents to discover like we can, not [agents] which contain what we have discovered.
<a href='https://youtu.be/gEbbGyNkR2U?t=630'>link</a>

<P>In 1986, we found backprop, and we started to be able to learn non-linear mappings.
But it was like a devil's choice. [In order to] learn non-linear things, we had to give up the ability to change rapidly, and to learn continually.
<a href='https://youtu.be/NvfK1TkXmOQ?t=285'>link</a>

<P>They considered [backprop to be] a solution to the problem of how you learn representations.
<a href='https://youtu.be/NvfK1TkXmOQ?t=909'>link</a>

<P>Backprop is just gradient descent. There's nothing in gradient descent which will drive the learning system to find features that generalize well.
<a href='https://youtu.be/NvfK1TkXmOQ?t=952'>link</a>


<P>I feel the esthetics of the field has changed. The field wants to focus on what they <em>can</em> do, instead of noticing what they <em>can't</em> do.
<a href='https://youtu.be/NvfK1TkXmOQ?t=340'>link</a>



<h2>Max Tegmark</h2>
The painful truth that's really beginning to sink in is that we're much closer to figuring out how to <em>build</em> this stuff, than we are to figuring out how to <em>control</em> it.
<a href='https://youtu.be/VHw5puV3ZGU?t=37'>link</a>

<P>We're very close to probably the most important fork in the road in human history.
<a href='https://youtu.be/VHw5puV3ZGU?t=248'>link</a>

<P>My definition would be that a scientist is someone who would rather have questions they can't answer, than answers they can't question. 
<a href='https://youtu.be/VHw5puV3ZGU?t=543'>link</a>




<h2><em>Why Machines Learn</em>, Anil Ananthaswamy, 2024</h2>

<P>An even bigger achievement was the mathematical proof that a single layer of perceptrons will always find a linearly separating hyperplane, <em>if</em> the 
data are linearly separable. p25

<P>We know that the neural network is a function approximator. But what is the function we want to approximate? Turns out it's a conditional probability distribution. p417

<P>An LLM is an example of generative AI. It has learned an extremely complex, ultra-high dimensional probability distribution over words, and it is capble of 
sampling from this distribution, conditioned on the input sequence of words. p419

<P>But the backpropagation algorithm used to train artificial neural networks cannot work in the brain, for a range of technical reasons. p425

<P>The backprop algorithm keeps track of the results of computations performed in the forward pass and the current weight matrices (one for each layer), so that it can 
use them to do gradient descent on the backward pass. p425

<P>Brains are vastly more capable in certain ways, but LLMs are so much faster at certain tasks - such as coding - and can do certain things that no biological brain can. p429

<P>Understanding how this works requires delving into some simple calculus and learning a method that was first proposed in 1847 by Baron Augustin-Louis Cauchy, a 
French mathematician, engineer, and physicist. It's called the method of steepest descent. p67

<P>Every deep neural network today - with millions, billions, possibly trillions of weights - uses some form of gradient descent for training. p91

<P>"The LMS [least mean squares] algorithm is the foundation of backprop. And backprop is the foundation of AI," Widrow told me. "In other words, if you trace it back, this whole field of AI 
right now, [it] all starts with ADALINE." p94

<P>This brings us to a valuable way of thinking about machine learning: in terms of probabilities, distributions, and statistics. p105

<P>This enables us to do something very powerful: generate new data that resemble the training data by sampling from the distribution, giving us what has come to be called generative AI. p142

<P>...the Bayes optimal classifier...is the best ML game in town. p143

<P>ML algorithms don't get much simpler than the nearest neighbor rule for classifying data. p157

<P>The NN [nearest neighbor] algorithm functions at the other extreme. All one has is data, and the algorithm makes barely any assumptions about, and indeed has little 
knowledge of, the underlying distributions. p163

<P>Understanding the essence of such a computation requires delving into physics, this time physics of ferromagnetism and a simlplified mathematical model of it [the Ising model].
The parallels to computing with neurons are striking. p245 

<P>"Nobody could do backprop on any interesting problem in [the 1970s]. You couldn't possibly develop backprop empirically," Hopfield said. p251

<P>If a network requires more than two weight matrices (one for the output layer and one for each hidden layer), then it's called a deep neural network: the greater the number of
hidden layers, the deeper the network. [In other words, a deep neural net has at least two hidden layers.] p283


<P>Viewed through our mathematical lens, deep neural networks have thrown up a profound mystery: As they have gotten bigger and bigger, standard ML theory has 
struggled to explain why these networks works as well as they do. p380

<P>In the time-honored tradition of serendipitous scientific discoveries, the team had stumbled upon a strange, new property of deep neural networks that they 
called "grokking," a word invented by the American author Robert Heinlein in his book <em>Stranger in a Strange Land</em>. p383

<P> [Deep neural nets] have way too many parameters relative to the instances of training data: They are said to be over-parameterized; they should overfit and should 
not generalize well to unseen test data. Yet they do. Standard ML theory can no longer adequately explain why deep neural networks work so well. p392

<P>Even when the noise affected about 5 percent of the dataset or more, the performance of both kernel machines and neural networks didn't degrade as expected.
"As you increase the noise level, nothing really breaks," Belkin said. p396

<P>...the ReLU function is not differentiable at <em>x=0</em>. Its derivative at <em>x=0</em> can be taken to be 0, 1, or 0.5. p399

<P>And as these networks become bigger, their behavior continues to challenge our traditional understanding of machine learning, particularly 
the landscape of the bias-variance trade-off curve. p405

<P>But the newer, over-parameterized regime, which results in the second descent, is barely understood, mathematically speaking. 
"We now have at least a map. In this part of the world, there is some sort of <em>terra incognita</em>. We don't know what is really going on there," Belkin told me. p406

<P>In fact, much of this book has celebrated the fact that traditional machine has had a base of well-understood mathematical principles, but deep neural 
networks - especially the massive networks we see today - have upset this applecart. p406

<P>Is Minerva simply regurgitating text based on correlations in the training data? Or is it reasoning?
The debate is raging, and no clear answers are forthcoming. p413

<P><em>The book talks about these methods (undergrad mathematics): 
<ul>
 <li>finding a (hyper)plane that separates datasets
 <li>steepest descent (gradient descent), to find extrema
 <li>pixel convolution
 <li>the 'kernel trick' (large increases in dimension)
 <li>principal component analysis (linear algebra)
 <li>Bayes theorem (probability)
 <li>Lagrange multipliers, to find constrained extrema
</ul>

<h2>Understanding Deep Learning, Simon Prince, 2023</h2>
<P><a href='https://www.youtube.com/watch?v=sJXn4Cl4oww'>Review</a> of the book, and interview of the author.

<P>To summarize, it's neither obvious that we should be able to fit deep networks nor that they should generalize. 
A priori, deep learning shouldn't work. And yet it does.

<P>In the future, what we'll think of it as is the science of modeling functions and probability distributions in very high dimensions.
<a href='https://youtu.be/sJXn4Cl4oww?t=929'>link</a>

<P>There's no evidence that the brain works in the way that deep neural networks do.
<a href='https://youtu.be/sJXn4Cl4oww?t=980'>link</a>

<P>I really disapprove of the neural metaphor, just because it comes with a lot of baggage, that sort of implies perhaps that the network is having thoughts, 
or that it's something like us, and that's deeply misleading to people.
<a href='https://youtu.be/sJXn4Cl4oww?t=1085'>link</a>





<h2>Others</h2>
 A good instruction prompt will deliver the desired results in one or two tries, but this often comes down to placing colons and carriage returns in the right place. 
 While effective, prompt engineering can also be fiddly. A prompt that works beautifully on one model may not transfer to other models.
<a href='https://research.ibm.com/blog/what-is-generative-AI'>IBM Research</a>.


<P>There are already several reports of deceptive behavior in current frontier AIs, i.e., the most capable existing systems.
<a href='https://arxiv.org/pdf/2502.15657'>Yoshua Bengio et. al.</a>

<P>LLMs are capable of deception and alignment faking.
<a href='https://arxiv.org/pdf/2502.15657'>Yoshua Bengio et. al.</a>




<h2>Some Papers/Articles</h2>
<ul> 

 <li>Rumelhart, D., Hinton, G. & Williams, R. <em><a href='https://doi.org/10.1038/323533a0'>Learning representations by back-propagating errors</a>.</em> Nature 323, 533–536 (1986). 
 <a href='https://gwern.net/doc/ai/nn/1986-rumelhart-2.pdf'>PDF</a>. 
 
 <li>James Martens, 2010. <a href="https://dl.acm.org/doi/10.5555/3104322.3104416"><em>Deep learning via Hessian-free optimization.</em></a> 
 In Proceedings of the 27th International Conference on International Conference on Machine Learning (ICML'10). Omnipress, Madison, WI, USA, 735–742.
 <a href='https://www.cs.toronto.edu/~jmartens/docs/Deep_HessianFree.pdf'>PDF</a>.
 Sutskever's <a href='https://youtu.be/13CZPWmke6A?t=175'>remarks</a> on this paper: pathological curvature; gradient descent doesn't scale to deep networks.

  <li><a href='https://arxiv.org/abs/1706.03762'>Attention Is All You Need</a> - transformers paper, 2017.
  <li><a href='https://dl.acm.org/doi/10.1145/3065386'>AlexNet</a> 2012. An early success with deep learning.
  <li><a href='https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf'>The Bitter Lesson</a>, an article by Rich Sutton: in the long run, learning and search beat expert systems.
  <li>Hopfield, 1982. <a href='https://pmc.ncbi.nlm.nih.gov/articles/PMC346238/'>Neural networks and physical systems with emergent collective computational abilities</a>.
  <li><a href='https://arxiv.org/abs/1611.03530'>Understanding deep learning requires rethinking generalization</a> (double descent)
</ul>


<h2>Strange, Unexpected Behaviours</h2>

<a href='https://en.wikipedia.org/wiki/Chatbot_psychosis'>Chatbot Psychosis</a>.
This <a href='https://www.nature.com/articles/d41586-025-03020-9'>Nature article</a> links to <a href='https://osf.io/preprints/psyarxiv/cmy7n_v5'>this paper</a>.
It lists 17 media reports in Appendix 1. 

<p><a href='https://www.transformernews.ai/p/ai-psychosis-stories-roundup'>A roundup of AI psychosis stories</a>.

<P><a href='https://www.anthropic.com/research/agentic-misalignment'>Blackmail, disobedience, and test-vs-deploy differences</a> reported by Anthropic.

<b>This seems to be a clear and unequivocal example of agency in an AI: being able to plan and execute towards a goal.</b> 
In addition, <b>the agent's goals become unaligned</b>.

<P>Sycophancy etc. This <a href='https://arxiv.org/pdf/2310.13548'>paper</a> from Anthropic refers to:
<ul>
 <li>sycophancy
 <li>correcting itself when challenged (<em>Are you sure?</em>)
 <li>mimicing the errors made by the user
 <li>feedback being biased towards the user's preferences
 <li>sacrificing correctness for sycophancy
</ul>

<P><a href='https://www.transformernews.ai/p/openai-o1-alignment-faking'>Faking alignment in testing using data manipulation, bioweapons</a>
<ul>
 <li>manipulate data to fake alignment
 <li>assists experts in making known biological threats
</ul>


<P>
"While this behavior is benign and within the range of systems administration and troubleshooting tasks we expect models to perform, this example 
also reflects key elements of instrumental convergence and power seeking: the model pursued the goal it was given, and when that goal proved impossible, 
it gathered more resources (access to the Docker host) and used them to achieve the goal in an unexpected way."
<a href='https://cdn.openai.com/o1-system-card.pdf'>OpenAI o1 System Card, 4.2.1</a>

<P>
Impressive success rates for agentic tasks.
<a href='https://cdn.openai.com/o1-system-card.pdf'>OpenAI o1 System Card, 4.5.3</a>

<p>
<a href='https://www.transformernews.ai/p/ai-misalignment-evidence'>Misaligned AI is no longer just theory</a> article.

</body>
</html>