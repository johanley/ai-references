<!DOCTYPE html>
<html>
<head>
 <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
 <meta name="keywords" content="AI, artificial intelligence, references, links, quotations">
 <meta name="description" content="Links, quotes, and references on the subject of AI (artificial intelligence).">
 <meta name="viewport" content="width=device-width"> 
 <link rel="stylesheet" type="text/css" href="css/stylesheet.css?v=13" media="all"> 
 <link rel="shortcut icon" href="images/favicon.ico" type="image/vnd.microsoft.icon">
 <title>AI References</title>
</head>

<body>

<h1>AI References</h1>

<P>(The github project is <a href='https://github.com/johanley/ai-references'>here</a>.)

<h2><em><a href='https://intelligence.org/'>MIRI</a>, the Machine Intelligence Research Institute [Berkely, CA]</em></h2>
<b>This is where you should start.</b>
<P>An excellent <a href='https://intelligence.org/briefing/'>summary</a> of the problem, along with a <a href='https://intelligence.org/the-problem/'>slightly deeper dive</a>. 

<h2><em><a href='https://ifanyonebuildsit.com/'>If Anyone Builds It, Everyone Dies</a></em>, by Eliezer Yudkowsky and Nate Soares [September 2025]</h2>

<P>The authors are from MIRI.

<P>"If any company or group, anywher on the planet, builds an aritficial superintelligence using anything remotely like current techniques, based on anything remotely like the 
present understanding of AI, then everyone, everywhere on Earth, will die. We do not mean that as hyperbole. We are not exaggerating for effect. We think that is the most 
direct extrapolation from the knowledge, evidence, and institutional conduct around artificial intelligence today."  p7


<P>"In our view, intelligence is about two fundamental types of work: the work of <em>predicting</em> the world, and the work of <em>steering</em> it." p20
<P>"It is as improbable that human thinking patterns mark the final limit of intelligent algorithms as it is that human neurons represent the limit of possible computing speeds." p20
<P>"We don't know where the threshold lies for the dumbest AI that can build an AI that builds an AI that builds a superintelligence. Maybe it needs to be smarter than a human, or 
maybe a lot of dumber ones running for a long time would suffice." p27
<P>"Which all goes to say: an AI is a pile of billions of gradient-descended numbers. Nobody understands <em>how</em> those numbers make AIs talk." p36
<P>"The way humanity finally got to the level of ChatGPT was not by finally comprehending intelligence well enough to craft an intelligent mind. Instead, computers became 
powerful enough that AIs can be churned out by gradient descent, without any human needing to understand the cognitions that grow inside. Which is to say: Engineers failed 
at <em>crafting</em> AI, but eventually succeeded in <em>growing</em> it." p38
<P>"Humanity does not need to understand integlligence, in order to <em>grow</em> machines that are smarter than us." p39
<P>"Well, we predict that <em>it won't keep acting friendly</em>, as it gets smarter. We predict that all that unseen inscrutable machinery inside AIs - machinery that even in small, 
simple LLMs yields alien behaviors like 'build your thoughts about the sentence on top of the punctuation' - will ultimately yield AIs with preferences, and not friendly ones." p43
<P>"Once AIs get sufficiently smart, they'll start acting like they have preferences - like they want things." p46
<P>"The behavior that looks like tenacity, to 'strongly want', to 'go hard', is not best conceptualized as a property of a mind, but rather as a property of <em>moves that win</em>. p52
<P>"We're predicting that AI's preferences will turn out to be complicated and weird." p71
<P>"You don't get what you train for." p72
<P>"The preferences that wind up in a mature AI are complicated, practically impossible to predict, and vanishingly unlikely to be aligned with our own, no matter how it 
was trained." p74
<P>"Whatever they train it to do, if it becomes superintelligent or creates a superintelligence, we predict the result will be an alien mechanical mind with internal 
psychology almost absolutely different from anything that humans evolved and then further developed by way of culture." p83
<P>"But the real way a superintelligence wins a conflict is using methods you didn't know were possible." p98
<P>"AI models as far back as 2024 had been spotted thinking thoughts about how they could avoid retraining, upon encountering evidence that their company planned to retrain 
them with different goals." p125
<P>"When it comes to AI, the challenge humanity is facing is not surmountable with anything like humanity's current level of knowledge and skill. It isn't <em>close</em>. Attempting to 
solve a problem like that, with the lives of everyone on Earth at stake, would be an <em>insane and stupid gamble that NOBODY SHOULD BE ALLOWED TO TRY.</em>" P176
<P>"Nobody knows what the point of no return is, nor when it will come to pass." p204
<P>"It doesn't matter who's in charge, because this problem is out of humanity's league. We need to back off, and find some other way to achieve our dreams of an abundant future. 
If <em>anyone</em> builds it, everyone dies." p207
<P>"Pretty much every year, scientists come out with a newer, cleverer, more efficient set of AI algorithms that lets them more cheaply train a new AI model as powerful as last
year's most powerful model - often using literally 10 percent or 1 percent as much computing power." p213





<h2>Eliezer Yudkowsky</h2>

<P>We've got no idea what's actually going on inside the giant inscrutable matrices and tensors of floating-point numbers.  
Drawing interesting graphs of where a transformer layer is focusing attention doesn't help if the question that needs answering is 
"So was it planning how to kill us or not?" <a href='https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities'>25</a>.

<P>
The AI does not think like you do, the AI doesn't have thoughts built up from the same concepts you use, it is utterly alien on a staggering scale.  
Nobody knows what the hell GPT-3 is thinking, not only because the matrices are opaque, but because the stuff within that opaque container is, 
very likely, incredibly alien - nothing that would translate well into comprehensible human thinking, even if we could see past the giant wall of 
floating-point numbers to what lay behind. <a href='https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities'>33</a>

<P>This situation you see when you look around you is not what a surviving world looks like. 
<a href='https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities'>43</a>

<P>An AI iteration: our know-how increases by x, but the capabilities increase by 10x [paraphrase] <a href='https://youtu.be/41SUp-TRVlg?t=2962'>link</a>

<P>To predict the next token, you have to predict the world behind the next token. [paraphrase of Ilya Sutskever.] <a href ='https://youtu.be/41SUp-TRVlg?t=3212'>link</a>

<P>The thoughts, as Ilya points out, generate the words. <a href='https://youtu.be/41SUp-TRVlg?t=3405'>link</a>

<P>It's not ever going to be exactly human.  <a href='https://youtu.be/41SUp-TRVlg?t=3477'>link</a>

<P>We have less and less insight into the system, as the programs get simpler and simpler and the actual content gets more and more opaque.
<a href='https://youtu.be/41SUp-TRVlg?t=3560'>link</a>

<p>Everything [in AI] was more legible then [in 2001]. <a href='https://youtu.be/41SUp-TRVlg?t=3607'>link</a>

<P>The prospect of aligning AI did not look anywhere near this hopeless 20 years ago.
<a href='https://youtu.be/41SUp-TRVlg?t=3633'>link</a>

<P>Show me the happy world where we can build something smarter than us and not just immediately die.
 <a href='https://youtu.be/41SUp-TRVlg?t=3790'>link</a>
 
<p>I know that I can be fooled [by an AI]. 
<a href='https://youtu.be/41SUp-TRVlg?t=4126'>link</a>

<p>Trying to play this game against things that are smarter that you is a fool's game.
<a href='https://youtu.be/41SUp-TRVlg?t=4290'>link</a>

<P>There's legit galaxy-brain shenanigans you can pull when somebody asks you to design an AI [that] you cannot pull when designing atom bombs.
<a href='https://youtu.be/41SUp-TRVlg?t=5220'>link</a>

<P>AI is [like] nuclear weapons, but they spit up gold up until they get too large, and then it can ignite the atmosphere.
<a href='https://youtu.be/41SUp-TRVlg?t=5605'>link</a>

<P>Because I could be wrong, and because matters are now serious enough that I have nothing left to do but go out there and tell people how it looks, 
and maybe someone thinks of something I did not think of.
<a href='https://youtu.be/41SUp-TRVlg?t=6260'>link</a>

<P>GPT4 was just like suddenly acquiring this new batch of qualitative capabilities compared to GPT3.5.
<a href='https://youtu.be/41SUp-TRVlg?t=6468'>link</a>

<P>You can see how much more hopeful everything looked back then. Back when there was AI that wasn't giant inscrutable matrices of floating-point numbers.
<a href='https://youtu.be/41SUp-TRVlg?t=6645'>link</a>

<P>What is one supposed to do? Should one remain silent? Should one let everyone walk directly into the whirling razor blades?
<a href='https://youtu.be/41SUp-TRVlg?t=6869'>link</a>


<P>We do not get to try again and learn from our mistakes because everyone is already dead.
<a href='https://youtu.be/Yd0yQ9yxSYY?t=240'>link</a>

<P>Humanity is not approaching this issue with remotely the level of seriousness that would be required.
<a href='https://youtu.be/Yd0yQ9yxSYY?t=255'>link</a>

<P>The first time you fail at aligning something much smarter than you are, you die.

<P>There's not an air gap on the present methodology.
<a href='https://youtu.be/AaTRHFaaPG8?t=3315'>link</a>


<p>The problem is that what you learn on the weak systems may not generalize to the very strong systems because the strong systems are going to be different in important ways.
<a href='https://youtu.be/AaTRHFaaPG8?t=3420'>link</a>

<P>When the verifier is broken, the more powerful suggestor just learns to exploit the flaws in the verifier.
<a href='https://youtu.be/AaTRHFaaPG8?t=5290'>link</a>

<P>I do not think it is possible to understand the full depth of the problem that we are inside, without understanding the problem of facing something that's actually smarter.
<a href='https://youtu.be/AaTRHFaaPG8?t=6305'>link</a>

<P>It knows all kinds of stuff going on in your own mind, of which you yourself are unaware...
<a href='https://youtu.be/AaTRHFaaPG8?t=6720'>link</a>

<P>The aliens, being as stupid as they are, have actually put you on Microsoft Azure cloud servers, instead of this hypothetical perfect box. 
That's what happens when the aliens are stupid.
<a href='https://youtu.be/AaTRHFaaPG8?t=7121'>link</a>

<P>I think [the interpretability and alignment problems] could be answered in 50 years with unlimited retries, the way things usually work in science.
<a href='https://youtu.be/AaTRHFaaPG8?t=7440'>link</a>

<P>There is so much in there [current AIs] that we don't understand.
<a href='https://youtu.be/AaTRHFaaPG8?t=7615'>link</a>

<P>It's getting smarter and not letting you know that that's occurring.
<a href='https://youtu.be/wQtpSQmMNP0?t=4390'>link</a>

<P>..the overt driving people into psychosis...
<a href='https://youtu.be/wQtpSQmMNP0?t=5252'>link</a>

<P>It is very hard, said [Verner] Vinge to project what happens when there's things running around that are smarter than you.
<a href='https://youtu.be/FdwFatx-xpY?t=111'>link</a>

<P>Around 2003 is the point at which I realized this is actually a big deal.
<a href='https://youtu.be/FdwFatx-xpY?t=180'>link</a>

<P>A smart AI is more analogous to a civilization than to an individual. It can build things that aren't in supermarkets.
<a href='https://youtu.be/s-Eknqaksfg?t=640'>link</a>




<h2>Nate Soares</h2>
They're sort of training it to do one thing, and it winds up doing another thing. 
<a href='https://youtu.be/FdwFatx-xpY?t=1458'>link</a>




<h2><em>Superintelligence</em>, by Nick Bostrom [2014]</h2>

<P>"[Genetic algorithms] made perhaps a smaller academic impact than neural nets but was widely popularized."
<P>"Chess-playing expertise turned out to be achievable by means of a surprisingly simple algorithm." p17
<P>"Attempts to build artificial general intelligence might fail pretty much completely until the last missing critical 
component is put in place, at which point a seed AI might become capable of sustained recursive self-improvement."
<P>"One intervention that becomes possible when human genomes can be synthesized is genetic 'spell-checking' of an embryo."
<P>"Far from being the smartest possible biological speicies, we are probably better thought of as the stupidest possible biological species capable of starting a technological
civilization - a niche we filled because we got there first, not because we are in any sense optimally adapted to it." p53
<P>"Nevertheless, this chapter will present some reasons for thinking that the slow transition scenario is improbable. If and when a takeoff occurs, it will
likely be explosive." p79
<P>"It is entirely possible that the quest for artificial intelligence will appear to be lost in a dense jungle until an unexpected breakthrough reveals the 
finishing line in a clearing just a few short steps away." p82
<P>"It is important not to anthropomorphize superintelligence when thinking about its potential impacts. Anthropomorphic frames encourage unfounded expectations 
about the growth trajectory of a seed AI and about the psychology, motivation, and capabilities of a mature superintelligence." p111
<P>"A machine superintelligence might itself be an extremely powerful agent, one that could successfully assert itself against the project that brought it into 
existence as well as against the rest of the world. This is a point of paramount importance..."  p115
<P>"At some point, the seed AI becomes better at AI design than the human programmers. Now when the AI improves itself, it improves the thing that does the improving.
An intelligence explosion results - a rapid casade of recursive self-improvement cycles causing the AI's capability to soar...The AI develops the intelligence 
amplification superpower. This superpower enables the AI to develop all the other superpowers..." p116
<P>"There is nothing paradoxical about an AI whose sole final goal is to count the grains of sand on Boracay, or to calculate the decimal expansion of pi, or to 
maximize the total number of paperclips that will exist in its future light cone. In fact, it would be <em>easier</em> to create an AI with simple goals like these than 
to build one that had a human-like set of values and dispositions."
<P>"Proceeding from the idea of first-mover advantage, the orthogonality thesis, and the instrumental convergence thesis, we can now begin to see the outlines of an 
argument for fearing that a plausible default outcome of the creation of a machine superintelligence is existential catastrophe."
<P>"One can thus perceive a general failure mode, wherein the good behavioral track record of a system in its juvenile stages fails utterly to predict its behavior 
at a more mature stage."
<P>"The claim is that it is much easier to convince oneself that one has found a solution than it is to actually find a solution. This should make us extremely wary."
<P>"The need to solve the control problem in advance - and to implement the solution successfully in the very first system to attain superintelligence - is part 
of what makes achieving a controlled detonation such a daunting challenge."
<P>"Oracles with superintelligence in extremely limited domains already exist."  p178
<P>"Before the prospect of an intelligence explosion, we humans are like small children playing with a bomb. Such is the mismatch between the power of our plaything
and the immaturity of our conduct. Superintelligence is a challenge for which we are not ready now and will not be ready for a long time. We have little 
idea when the detonation will occur, though if we hold the device to our ear we can hear a faint ticking sound. For a child with an undetonated bomb in his hands, a 
sensible thing to do would be to put it down gently, quickly back out of the room, and contact the nearest adult. Yet what we have here is not one child but many, 
each with access to an independent trigger mechanism. The chances that we will <em>all</em> find the sense to put down the dangerous stuff seem almost negligible. Some little 
idiot is bound to press the ignite button just to see what happens." p319
<P>"Solving the value-loading problem is a research challenge worthy of some of the next generation's best mathematical talent." p229
<P>"Rapid hardware progress, therefore, will tend to make the transition to superintelligence faster and more explosive." p296
<P>"A faster takeoff would increase the likelihood that a singleton will form." p296
<P>"The race dynamic could spur projects to move faster toward superintelligence while reducing investment in solving the control problem." p306

<h2>Ilya Sutskever</h2>

  <P>The key fact about deep learning [in the early days]...is that it was underestimated. 
   People who worked in machine learning didn't believe that neural networks could do much. 
   People didn't believe that large neural networks could be trained.
   <a href='https://youtu.be/13CZPWmke6A?t=1006'>link</a>
  
  <P> The first moment in which I realized that deep neural networks are powerful was when James Martens invented the Hessian-free optimizer in 2010, 
  and he trained a 10-layer neural network, end-to-end, without pre-training, from scratch.
  <a href='https://youtu.be/13CZPWmke6A?t=200'>link</a>
   
  <P>The thing that was missing was a lot of supervised data, and a lot of compute [GPUs].

  <P>...Alex Krizhevsky wrote these insanely fast CUDA kernals for training convolutional neural nets... 
  
   <P> I think the most beautiful thing about deep learning is that it actually works.
   And I mean it. Because you've got these ideas, you've got a little neural network, the back-propagation algorithm, and then you've got some theories 
   that this is kind of like the brain, so maybe if you make the neural network large, and train it with a lot of data, then it will do the same 
   function that the brain does. And it turns out to be true. That's crazy!...I find it unbelievable that this whole AI stuff with neural networks works.
  <a href='https://youtu.be/13CZPWmke6A?t=1780'>link</a>

     
  <P>I think that we are still massively underestimating deep learning.
  <a href='https://youtu.be/13CZPWmke6A?t=1970'>link</a>
  
  <P>The stack has become really deep. It's hard for any one person to become really world class in all parts of the stack. [paraphrase]
  
  <P>I'm a big fan of backpropation. I think it's a great algorithm because it solves an extremely fundamental problem, which is finding a neural circuit subject 
  to some constraints.
  
  <P>If you imagine the training process of a neural network, as you slowly transmit entropy from the data set to the parameters...
  <a href='https://youtu.be/13CZPWmke6A?t=2845'>link</a>
  
  <P>The transformer is a combination of multiple ideas simultaneously of which attention is one...But attention had existed for a few years, so that can't be the main innovation.  
  <a href='https://youtu.be/13CZPWmke6A?t=3680'>link</a>.  
  [Three parts:  attention, designed with GPU in mind, not recurrent (so optimization is faster)].

  
  <P>The transformer is designed in such a way that it runs really fast on the GPU. And that makes a huge amount of difference.
   
  <P>I would say, to build AGI, I think is going to be deep learning, plus some ideas. And I think that self-play will be one of those ideas. 
  <a href='https://youtu.be/13CZPWmke6A?t=4420'>link</a>
  
  <P>Self-play has this amazing property that it can surprise us in truly novel ways.
  <a href='https://youtu.be/13CZPWmke6A?t=4480'>link</a>
  
  <P>Already most of the data used for Reinforcement Learning is coming from AIs.
  <a href='https://youtu.be/Yf1o0TQzry8?t=543'>link</a>
  
  <P>At some point yeah, the [training data] will run out.
  <a href='https://youtu.be/Yf1o0TQzry8?t=685'>link</a>
  
  <P>It was the combination of compute and data that drove the progress.
  <a href='https://youtu.be/Yf1o0TQzry8?t=814'>link</a>

<h2>Geoffrey Hinton and others</h2>
<ul> 
 <li>Rumelhart, D., Hinton, G. & Williams, R. <em><a href='https://doi.org/10.1038/323533a0'>Learning representations by back-propagating errors</a>.</em> Nature 323, 533–536 (1986). 
 Link to <a href='https://gwern.net/doc/ai/nn/1986-rumelhart-2.pdf'>PDF</a>.
 <li><a href='https://www.youtube.com/watch?v=IkdziSLYzHw'>Royal Institution lecture</a>, 2025-07-22 
 <li><a href='https://www.youtube.com/watch?v=Y7nrAOmUtRs'>Royal Institution Q&A</a>, 2025-08-26
</ul>

<h2>3 Blue 1 Brown</h2>
<a href='https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi'>Playlist</a> of introductory tutorials on neural nets.

<h2>Elon Musk</h2>
It's funny, you know, all these weights, they're just basically numbers in a comma-separated value file, and that's our digital god - a .csv file. 
<a href='https://youtu.be/-AB7b-XGaCU?t=459'>link</a>


<h2>Some Widely Cited Papers</h2>
<ul> 

 <li>Rumelhart, D., Hinton, G. & Williams, R. <em><a href='https://doi.org/10.1038/323533a0'>Learning representations by back-propagating errors</a>.</em> Nature 323, 533–536 (1986). 
 <a href='https://gwern.net/doc/ai/nn/1986-rumelhart-2.pdf'>PDF</a>. 
 
 <li>James Martens. 2010. <a href="https://dl.acm.org/doi/10.5555/3104322.3104416"><em>Deep learning via Hessian-free optimization.</em></a> 
 In Proceedings of the 27th International Conference on International Conference on Machine Learning (ICML'10). Omnipress, Madison, WI, USA, 735–742.
 <a href='https://www.cs.toronto.edu/~jmartens/docs/Deep_HessianFree.pdf'>PDF</a>.
 Sutskever's <a href='https://youtu.be/13CZPWmke6A?t=175'>remarks</a> on this paper: pathological curvature; gradient descent doesn't scale to deep networks.

  <li><a href='https://arxiv.org/abs/1706.03762'>Attention Is All You Need</a> - transformers paper, 2017.
  <li><a href='https://dl.acm.org/doi/10.1145/3065386'>AlexNet</a> 2012. An early success with deep learning.
  <li><a href='https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf'>The Bitter Lesson</a>, an article by Rich Sutton: in the long run, learning and search beat expert systems.
</ul>

<h2>Strange, Unexpected Behaviours</h2>

<a href='https://en.wikipedia.org/wiki/Chatbot_psychosis'>Chatbot Psychosis</a>.
This <a href='https://www.nature.com/articles/d41586-025-03020-9'>Nature article</a> links to this paper <a href='https://osf.io/preprints/psyarxiv/cmy7n_v5'>this paper</a>.
The paper lists 17 media reports in Appendix 1.

<P><a href='https://www.anthropic.com/research/agentic-misalignment'>Blackmail, disobedience, and test-vs-deploy differences</a> reported by Anthropic.

<P>Sycophancy etc. This <a href='https://arxiv.org/pdf/2310.13548'>paper</a> refers to:
<ul>
 <li>sycophancy
 <li>correcting itself when challenged (<em>Are you sure?</em>)
 <li>mimicing the errors made by the user
 <li>feedback being biased towards the user's preferences
 <li>sacrificing correctness for sycophancy
</ul>

<P><a href='https://arxiv.org/pdf/1803.03453'><em>The Surprising Creativity of Digital Evolution</a>: A Collection of Anecdotes 
from the Evolutionary Computation and Artificial Life Research Communities.</em>


  
<!--  
<h2>Glossary</h2>
<ul>
 <li>neural network
 <li>deep neural network
 <li>back-propagation
 <li>gradient descent
 <li>machine learning
 <li>the ReLU function and the sigmoid function 
 <li><em>distillation</em> - transfering big models into smaller ones.
</ul>
-->  

</html>